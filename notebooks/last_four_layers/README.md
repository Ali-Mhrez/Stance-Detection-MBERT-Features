# Results: Using Last Four Layer Features

In this experiment, we explored a deeper level of representation by concatenating the features from the last four layers of the pre-trained Multilingual BERT model. This approach aimed to capture a wider range of contextual information, from fine-grained details to broader semantic meanings. The resulting feature vectors were then fed into Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) architectures for Stance Detection.

By leveraging a richer representation, we investigate the potential for improved performance in capturing complex linguistic nuances and identifying subtle stance cues.
